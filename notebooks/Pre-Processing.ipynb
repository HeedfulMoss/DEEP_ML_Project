{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b4c367-d830-4834-be7e-b97d7c6eae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\\extracted\\D_ICD_DIAGNOSES.csv\n",
      "..\\data\\extracted\\DIAGNOSES_ICD.csv\n",
      "..\\data\\extracted\\NOTEEVENTS.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# username = os.getlogin()  # Works locally\n",
    "# desc_path = os.path.join(\"C:\\\\Users\", username, \"Documents\", \"GitHub\", \n",
    "#                          \"DEEP_ML_Project\", \"data\", \"extracted\" , \"D_ICD_DIAGNOSES.csv\")\n",
    "# diag_path = os.path.join(\"C:\\\\Users\", username, \"Documents\", \"GitHub\", \n",
    "#                          \"DEEP_ML_Project\", \"data\", \"extracted\" , \"DIAGNOSES_ICD.csv\")\n",
    "# notes_path = os.path.join(\"C:\\\\Users\", username, \"Documents\", \"GitHub\", \n",
    "#                          \"DEEP_ML_Project\", \"data\", \"extracted\" , \"NOTEEVENTS.csv\")\n",
    "\n",
    "extracted_dir = Path(\"../data/extracted\")\n",
    "desc_path = extracted_dir / \"D_ICD_DIAGNOSES.csv\"\n",
    "diag_path = extracted_dir / \"DIAGNOSES_ICD.csv\"\n",
    "notes_path = extracted_dir / \"NOTEEVENTS.csv\"\n",
    "\n",
    "print(desc_path)\n",
    "print(diag_path)\n",
    "print(notes_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "092f8025-de0c-45ea-b262-b77dcc395d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define improved section extraction functions\n",
    "def identify_sections(text):\n",
    "    \"\"\"\n",
    "    Identify common section headers in MIMIC-III discharge summaries\n",
    "    Returns a dictionary of section names and their starting positions\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return {}\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Common section headers in MIMIC-III discharge summaries\n",
    "    # Format: (regex pattern, standardized name)\n",
    "    section_patterns = [\n",
    "        # Common discharge summary sections\n",
    "        (r'(?i)(?:admission date|date of admission)\\s*:', 'ADMISSION_DATE'),\n",
    "        (r'(?i)(?:discharge date|date of discharge)\\s*:', 'DISCHARGE_DATE'),\n",
    "        (r'(?i)date of birth\\s*:', 'DATE_OF_BIRTH'),\n",
    "        (r'(?i)sex\\s*:', 'SEX'),\n",
    "        (r'(?i)service\\s*:', 'SERVICE'),\n",
    "        (r'(?i)(?:chief\\s+complaint|reason for admission|reason for hospitalization)\\s*:', 'CHIEF_COMPLAINT'),\n",
    "        (r'(?i)(?:history of (?:the )?present illness|hpi)\\s*:', 'HISTORY_OF_PRESENT_ILLNESS'),\n",
    "        (r'(?i)(?:brief )?(?:hospital course|hospital stay)\\s*:', 'HOSPITAL_COURSE'),\n",
    "        (r'(?i)(?:past medical history|pmh|medical history)\\s*:', 'PAST_MEDICAL_HISTORY'),\n",
    "        (r'(?i)(?:past surgical history|surgical history)\\s*:', 'SURGICAL_HISTORY'),\n",
    "        (r'(?i)preoperative diagnos(?:is|es)\\s*:', 'PREOP_DIAGNOSIS'),\n",
    "        (r'(?i)postoperative diagnos(?:is|es)\\s*:', 'POSTOP_DIAGNOSIS'),\n",
    "        (r'(?i)(?:admission|interim) diagnos(?:is|es)\\s*:', 'ADMISSION_DIAGNOSIS'),\n",
    "        (r'(?i)(?:final diagnos(?:is|es)|discharge diagnos(?:is|es))\\s*:', 'DISCHARGE_DIAGNOSIS'),\n",
    "        (r'(?i)(?:condition (?:upon|on) discharge|discharge condition)\\s*:', 'DISCHARGE_CONDITION'),\n",
    "        (r'(?i)discharge disposition\\s*:', 'DISCHARGE_DISPOSITION'),\n",
    "        (r'(?i)discharge (?:instructions|status)\\s*:', 'DISCHARGE_INSTRUCTIONS'),\n",
    "        (r'(?i)(?:discharge )?medications(?: on discharge| at discharge)?\\s*:', 'DISCHARGE_MEDICATIONS'),\n",
    "        (r'(?i)medications(?: on admission)?\\s*:', 'ADMISSION_MEDICATIONS'),\n",
    "        (r'(?i)current medications\\s*:', 'CURRENT_MEDICATIONS'),\n",
    "        (r'(?i)allergies\\s*:', 'ALLERGIES'),\n",
    "        (r'(?i)physical (?:examination|exam)\\s*:', 'PHYSICAL_EXAMINATION'),\n",
    "        (r'(?i)review of systems\\s*:', 'REVIEW_OF_SYSTEMS'),\n",
    "        (r'(?i)(?:social history|social)\\s*:', 'SOCIAL_HISTORY'),\n",
    "        (r'(?i)(?:family history|fh)\\s*:', 'FAMILY_HISTORY'),\n",
    "        (r'(?i)laboratory(?: data| results| values)?\\s*:', 'LABORATORY_DATA'),\n",
    "        (r'(?i)imaging\\s*:', 'IMAGING'),\n",
    "        (r'(?i)imaging studies\\s*:', 'IMAGING_STUDIES'),\n",
    "        (r'(?i)radiology\\s*:', 'RADIOLOGY'),\n",
    "        (r'(?i)(?:procedures performed|procedures)\\s*:', 'PROCEDURES'),\n",
    "        (r'(?i)assessment\\s*(?:and plan)?:', 'ASSESSMENT'),\n",
    "        (r'(?i)assessment and plan\\s*:', 'ASSESSMENT_AND_PLAN'),\n",
    "        (r'(?i)(?:plan|plans)\\s*:', 'PLAN'),\n",
    "        (r'(?i)consultations\\s*:', 'CONSULTATIONS'),\n",
    "        (r'(?i)follow(?:[-\\s])?up\\s*:', 'FOLLOWUP'),\n",
    "        (r'(?i)disposition\\s*:', 'DISPOSITION'),\n",
    "        (r'(?i)code status\\s*:', 'CODE_STATUS'),\n",
    "        (r'(?i)impression\\s*:', 'IMPRESSION'),\n",
    "        (r'(?i)findings\\s*:', 'FINDINGS'),\n",
    "        \n",
    "        # Additional formats sometimes found in MIMIC-III\n",
    "        (r'(?i)^chief complaint:\\s*', 'CHIEF_COMPLAINT'),  # At beginning of line\n",
    "        (r'(?i)^history of present illness:\\s*', 'HISTORY_OF_PRESENT_ILLNESS'),\n",
    "        (r'(?i)^hospital course:\\s*', 'HOSPITAL_COURSE'),\n",
    "        (r'(?i)^discharge diagnosis:\\s*', 'DISCHARGE_DIAGNOSIS'),\n",
    "        (r'(?i)^past medical history:\\s*', 'PAST_MEDICAL_HISTORY'),\n",
    "        (r'(?i)^assessment:\\s*', 'ASSESSMENT'),\n",
    "        (r'(?i)^impression:\\s*', 'IMPRESSION'),\n",
    "    ]\n",
    "    \n",
    "    # Find all section positions\n",
    "    section_positions = {}\n",
    "    \n",
    "    for pattern, name in section_patterns:\n",
    "        for match in re.finditer(pattern, text):\n",
    "            # Store the position of the section header\n",
    "            section_positions[match.start()] = name\n",
    "    \n",
    "    # Sort by position\n",
    "    return {pos: section_positions[pos] for pos in sorted(section_positions.keys())}\n",
    "\n",
    "def extract_sections(text):\n",
    "    \"\"\"\n",
    "    Extract sections from a discharge summary\n",
    "    Returns a dictionary with section names as keys and section text as values\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return {}\n",
    "    \n",
    "    # Identify sections and their positions\n",
    "    section_positions = identify_sections(text)\n",
    "    positions = list(section_positions.keys())\n",
    "    \n",
    "    # If no sections found, return the entire text as 'FULL_TEXT'\n",
    "    if not positions:\n",
    "        return {'FULL_TEXT': text.strip()}\n",
    "    \n",
    "    # Extract text for each section\n",
    "    sections = {}\n",
    "    \n",
    "    for i, pos in enumerate(positions):\n",
    "        section_name = section_positions[pos]\n",
    "        \n",
    "        # Find the end of this section (start of next section or end of text)\n",
    "        if i < len(positions) - 1:\n",
    "            section_end = positions[i + 1]\n",
    "        else:\n",
    "            section_end = len(text)\n",
    "        \n",
    "        # Extract section text (skip the header)\n",
    "        # Find the end of the header (look for colon)\n",
    "        colon_pos = text.find(':', pos)\n",
    "        if colon_pos != -1 and colon_pos < section_end:\n",
    "            header_end = colon_pos + 1\n",
    "        else:\n",
    "            # If no colon found, look for newline\n",
    "            nl_pos = text.find('\\n', pos)\n",
    "            if nl_pos != -1 and nl_pos < section_end:\n",
    "                header_end = nl_pos + 1\n",
    "            else:\n",
    "                # If neither found, use position + length of pattern as approximation\n",
    "                header_end = pos + 30  # Approximate header length\n",
    "        \n",
    "        section_text = text[header_end:section_end].strip()\n",
    "        \n",
    "        # Store the extracted section\n",
    "        sections[section_name] = section_text\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def preprocess_mimic_text(text):\n",
    "    \"\"\"\n",
    "    Apply comprehensive preprocessing to MIMIC-III text\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Replace de-identified elements\n",
    "    text = re.sub(r'\\[\\*\\*\\d{4}-\\d{1,2}-\\d{1,2}\\*\\*\\]', '[DATE]', text)\n",
    "    text = re.sub(r'\\[\\*\\*\\d{1,2}/\\d{1,2}/\\d{2,4}\\*\\*\\]', '[DATE]', text)  # MM/DD/YYYY format\n",
    "    text = re.sub(r'\\[\\*\\*\\d{4}\\*\\*\\]', '[YEAR]', text)\n",
    "    text = re.sub(r'\\[\\*\\*.*?[Hh]ospital.*?\\*\\*\\]', '[HOSPITAL]', text)\n",
    "    text = re.sub(r'\\[\\*\\*.*?[Cc]linic.*?\\*\\*\\]', '[CLINIC]', text)\n",
    "    text = re.sub(r'\\[\\*\\*.*?[Dd]octor.*?\\*\\*\\]', '[DOCTOR]', text)\n",
    "    text = re.sub(r'\\[\\*\\*.*?[Nn]ame.*?\\*\\*\\]', '[NAME]', text)\n",
    "    text = re.sub(r'\\[\\*\\*.*?[Pp]hone.*?\\*\\*\\]', '[PHONE]', text)\n",
    "    text = re.sub(r'\\[\\*\\*.*?[Ss]tate.*?\\*\\*\\]', '[STATE]', text)\n",
    "    text = re.sub(r'\\[\\*\\*.*?[Cc]ity.*?\\*\\*\\]', '[CITY]', text)\n",
    "    text = re.sub(r'\\[\\*\\*.*?[Nn]umber.*?\\*\\*\\]', '[NUMBER]', text)\n",
    "    text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '[DEIDENTIFIED]', text)\n",
    "    \n",
    "    # Normalize spacing\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def create_clinical_weighted_text(sections):\n",
    "    \"\"\"\n",
    "    Create a weighted clinical text by combining the most predictive sections\n",
    "    Returns a focused text with the most diagnostically relevant information\n",
    "    \"\"\"\n",
    "    if not sections:\n",
    "        return \"\"\n",
    "    \n",
    "    weighted_pieces = []\n",
    "    \n",
    "    # Add the most diagnostically relevant sections\n",
    "    if 'CHIEF_COMPLAINT' in sections and sections['CHIEF_COMPLAINT']:\n",
    "        weighted_pieces.append(f\"CHIEF COMPLAINT: {sections['CHIEF_COMPLAINT']}\")\n",
    "        \n",
    "    if 'HISTORY_OF_PRESENT_ILLNESS' in sections and sections['HISTORY_OF_PRESENT_ILLNESS']:\n",
    "        weighted_pieces.append(f\"HPI: {sections['HISTORY_OF_PRESENT_ILLNESS']}\")\n",
    "        \n",
    "    if 'DISCHARGE_DIAGNOSIS' in sections and sections['DISCHARGE_DIAGNOSIS']:\n",
    "        weighted_pieces.append(f\"DIAGNOSIS: {sections['DISCHARGE_DIAGNOSIS']}\")\n",
    "    \n",
    "    if 'ASSESSMENT' in sections and sections['ASSESSMENT']:\n",
    "        weighted_pieces.append(f\"ASSESSMENT: {sections['ASSESSMENT']}\")\n",
    "    elif 'ASSESSMENT_AND_PLAN' in sections and sections['ASSESSMENT_AND_PLAN']:\n",
    "        weighted_pieces.append(f\"ASSESSMENT: {sections['ASSESSMENT_AND_PLAN']}\")\n",
    "        \n",
    "    if 'IMPRESSION' in sections and sections['IMPRESSION']:\n",
    "        weighted_pieces.append(f\"IMPRESSION: {sections['IMPRESSION']}\")\n",
    "    \n",
    "    # Add hospital course (contains important clinical details)\n",
    "    if 'HOSPITAL_COURSE' in sections and sections['HOSPITAL_COURSE']:\n",
    "        weighted_pieces.append(f\"COURSE: {sections['HOSPITAL_COURSE']}\")\n",
    "    \n",
    "    return \" \".join(weighted_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30071181-52d7-48c2-ad7e-d63ac98e1f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x131fe0dd4f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an in-memory DuckDB connection\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "# Load CSVs directly using DuckDB (efficient for large files)\n",
    "query = f\"\"\"\n",
    "-- Load NOTEEVENTS (discharge summaries only, no known errors)\n",
    "CREATE VIEW noteevents AS\n",
    "SELECT *\n",
    "FROM read_csv_auto('{notes_path}')\n",
    "WHERE category = 'Discharge summary' AND ISERROR IS NULL;\n",
    "\n",
    "-- Load DIAGNOSES_ICD\n",
    "CREATE VIEW diagnoses_icd AS\n",
    "SELECT *\n",
    "FROM read_csv_auto('{diag_path}');\n",
    "\n",
    "-- Load ICD-9 descriptions\n",
    "CREATE VIEW d_icd_diagnoses AS\n",
    "SELECT *\n",
    "FROM read_csv_auto('{desc_path}');\n",
    "\"\"\"\n",
    "\n",
    "# Run the multi-query\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01437c52-5866-4114-94ad-2361c6318a17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display NOTEEVENTS column names and top 5 rows\n",
    "#print(\"NOTEEVENTS Table Schema:\")\n",
    "#noteevents_schema = con.execute(\"PRAGMA table_info(noteevents);\").fetchdf()\n",
    "#print(noteevents_schema[['name']])  # Display only column names\n",
    "\n",
    "#print(\"\\nTop 5 rows from NOTEEVENTS:\")\n",
    "#noteevents_preview = con.execute(\"SELECT * FROM noteevents LIMIT 5;\").fetchdf()\n",
    "#print(noteevents_preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac888b27-410c-46f3-a3c8-3266caf13efa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top codes columns: ['ICD9_CODE', 'count']\n",
      "Descriptions columns: ['ROW_ID', 'ICD9_CODE', 'SHORT_TITLE', 'LONG_TITLE']\n"
     ]
    }
   ],
   "source": [
    "# Get top 20 most frequent ICD-9 codes\n",
    "top_codes_query = f\"\"\"\n",
    "SELECT icd9_code, COUNT(*) as count\n",
    "FROM read_csv_auto('{diag_path}')\n",
    "GROUP BY icd9_code\n",
    "ORDER BY count DESC\n",
    "LIMIT 20;\n",
    "\"\"\"\n",
    "top_codes_df = con.execute(top_codes_query).fetchdf()\n",
    "top_codes = top_codes_df['ICD9_CODE'].tolist()\n",
    "#print(top_codes)\n",
    "\n",
    "# Load ICD-9 descriptions into a DataFrame\n",
    "desc_df = con.execute(\"SELECT * FROM d_icd_diagnoses\").fetchdf()\n",
    "\n",
    "# Optional: check column names\n",
    "print(\"Top codes columns:\", top_codes_df.columns.tolist())\n",
    "print(\"Descriptions columns:\", desc_df.columns.tolist())\n",
    "\n",
    "# Try both UPPER and lowercase just in case\n",
    "count_col = 'COUNT' if 'COUNT' in top_codes_df.columns else 'count'\n",
    "\n",
    "# Merge with description\n",
    "merged_df = pd.merge(top_codes_df, desc_df[['ICD9_CODE', 'LONG_TITLE']], on='ICD9_CODE', how='left')\n",
    "\n",
    "# Sort and print\n",
    "merged_df = merged_df.sort_values(by=count_col, ascending=False).reset_index(drop=True)\n",
    "\n",
    "# for idx, row in merged_df.iterrows():\n",
    "#     print(f\"{idx+1}. Code: {row['ICD9_CODE']} → {row['LONG_TITLE']} → {row[count_col]} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c17a00-f318-425f-bef2-586ef5165886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469ffd4dde8d40239caa25f814a80830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 46614 records with one-hot encoded diagnoses\n"
     ]
    }
   ],
   "source": [
    "# Join, filter for discharge notes, keep only top codes\n",
    "fetch_query = f\"\"\"\n",
    "WITH joined AS (\n",
    "    SELECT \n",
    "        n.subject_id,\n",
    "        n.hadm_id,\n",
    "        n.text AS summary_snippet,\n",
    "        d.icd9_code,\n",
    "        icd.long_title\n",
    "    FROM read_csv_auto('{notes_path}') n\n",
    "    JOIN read_csv_auto('{diag_path}') d\n",
    "      ON n.subject_id = d.subject_id AND n.hadm_id = d.hadm_id\n",
    "    JOIN read_csv_auto('{desc_path}') icd\n",
    "      ON d.icd9_code = icd.icd9_code\n",
    "    WHERE n.category = 'Discharge summary'\n",
    "      AND d.icd9_code IN ('{(\"', '\").join(top_codes)}')\n",
    ")\n",
    "SELECT \n",
    "    subject_id,\n",
    "    hadm_id,\n",
    "    MAX(summary_snippet) AS summary_snippet,  \n",
    "    STRING_AGG(icd9_code, ', ') AS icd9_codes,\n",
    "    STRING_AGG(long_title, '; ') AS diagnoses\n",
    "FROM joined\n",
    "GROUP BY subject_id, hadm_id\n",
    "\"\"\"\n",
    "diagnoses_df = con.execute(fetch_query).fetchdf()\n",
    "\n",
    "# Add diagnosis text fields and full summary text\n",
    "diagnoses_df['key'] = diagnoses_df['SUBJECT_ID'].astype(str) + '_' + diagnoses_df['HADM_ID'].astype(str)\n",
    "\n",
    "# Create a base dataframe for results with unique patient admissions\n",
    "base_results = diagnoses_df[['SUBJECT_ID', 'HADM_ID', 'key', 'summary_snippet', 'icd9_codes', 'diagnoses']].copy()\n",
    "\n",
    "# Create one-hot encoding columns initialized to 0\n",
    "for code in top_codes:\n",
    "    base_results[f'ICD9_{code}'] = 0\n",
    "\n",
    "# Populate the one-hot encoding columns based on the icd9_codes column\n",
    "for idx, row in base_results.iterrows():\n",
    "    if pd.notna(row['icd9_codes']):  # Check if icd9_codes is not NaN\n",
    "        patient_codes = [code.strip() for code in row['icd9_codes'].split(',')]\n",
    "        \n",
    "        # Set 1 for each code that the patient has\n",
    "        for code in patient_codes:\n",
    "            if code in top_codes:  # Only process codes in our top list\n",
    "                base_results.at[idx, f'ICD9_{code}'] = 1\n",
    "\n",
    "print(f\"Dataset contains {len(base_results)} records with one-hot encoded diagnoses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a63696b2-7224-4464-8890-04caea4b2148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text preprocessing and section extraction...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m base_results[\u001b[33m'\u001b[39m\u001b[33msummary_snippet_clean\u001b[39m\u001b[33m'\u001b[39m] = base_results[\u001b[33m'\u001b[39m\u001b[33msummary_snippet\u001b[39m\u001b[33m'\u001b[39m].apply(preprocess_mimic_text)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Step 2: Extract sections from each note\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m all_sections = \u001b[43mbase_results\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msummary_snippet_clean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_sections\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Calculate section statistics\u001b[39;00m\n\u001b[32m     11\u001b[39m section_counts = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\DEEP_ML_Project\\venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\DEEP_ML_Project\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\DEEP_ML_Project\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\DEEP_ML_Project\\venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\DEEP_ML_Project\\venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mextract_sections\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Identify sections and their positions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m section_positions = \u001b[43midentify_sections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m positions = \u001b[38;5;28mlist\u001b[39m(section_positions.keys())\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# If no sections found, return the entire text as 'FULL_TEXT'\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36midentify_sections\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     68\u001b[39m section_positions = {}\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pattern, name \u001b[38;5;129;01min\u001b[39;00m section_patterns:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     72\u001b[39m         \u001b[38;5;66;03m# Store the position of the section header\u001b[39;00m\n\u001b[32m     73\u001b[39m         section_positions[match.start()] = name\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Sort by position\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\pyver\\py3127\\Lib\\re\\__init__.py:219\u001b[39m, in \u001b[36mfinditer\u001b[39m\u001b[34m(pattern, string, flags)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[32m    211\u001b[39m \n\u001b[32m    212\u001b[39m \u001b[33;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m \n\u001b[32m    216\u001b[39m \u001b[33;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags).findall(string)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfinditer\u001b[39m(pattern, string, flags=\u001b[32m0\u001b[39m):\n\u001b[32m    220\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return an iterator over all non-overlapping matches in the\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[33;03m    string.  For each match, the iterator returns a Match object.\u001b[39;00m\n\u001b[32m    222\u001b[39m \n\u001b[32m    223\u001b[39m \u001b[33;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags).finditer(string)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ---------- SECTION EXTRACTION ENHANCEMENTS ----------\n",
    "print(\"Applying text preprocessing and section extraction...\")\n",
    "\n",
    "# Step 1: Apply basic preprocessing to clean the text\n",
    "base_results['summary_snippet_clean'] = base_results['summary_snippet'].apply(preprocess_mimic_text)\n",
    "\n",
    "# Step 2: Extract sections from each note\n",
    "all_sections = base_results['summary_snippet_clean'].apply(extract_sections)\n",
    "\n",
    "# Calculate section statistics\n",
    "section_counts = {}\n",
    "for sections in all_sections:\n",
    "    for section in sections:\n",
    "        section_counts[section] = section_counts.get(section, 0) + 1\n",
    "\n",
    "# Get top 15 most common sections\n",
    "top_sections = sorted(section_counts.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "print(\"\\nTop 15 most common sections in the dataset:\")\n",
    "for section, count in top_sections:\n",
    "    percentage = (count / len(base_results)) * 100\n",
    "    print(f\"{section}: {count} occurrences ({percentage:.2f}%)\")\n",
    "\n",
    "# Step 3: Extract key clinical sections\n",
    "key_sections = [\n",
    "    'CHIEF_COMPLAINT', \n",
    "    'HISTORY_OF_PRESENT_ILLNESS', \n",
    "    'HOSPITAL_COURSE',\n",
    "    'PAST_MEDICAL_HISTORY', \n",
    "    'DISCHARGE_DIAGNOSIS',\n",
    "    'ASSESSMENT',\n",
    "    'ASSESSMENT_AND_PLAN',\n",
    "    'IMPRESSION'\n",
    "]\n",
    "\n",
    "# Add columns for key sections\n",
    "for section in key_sections:\n",
    "    base_results[f'section_{section}'] = all_sections.apply(\n",
    "        lambda x: x.get(section, '') if isinstance(x, dict) else ''\n",
    "    )\n",
    "    \n",
    "    # Add binary indicator for section presence\n",
    "    base_results[f'has_{section}'] = base_results[f'section_{section}'].apply(\n",
    "        lambda x: 1 if isinstance(x, str) and len(x.strip()) > 0 else 0\n",
    "    )\n",
    "\n",
    "# Step 4: Create a weighted clinical text focusing on the most predictive sections\n",
    "base_results['clinical_weighted_text'] = all_sections.apply(create_clinical_weighted_text)\n",
    "\n",
    "# Step 5: Add diagnosis count \n",
    "base_results['diagnosis_count'] = base_results['icd9_codes'].apply(\n",
    "    lambda x: len(x.split(',')) if isinstance(x, str) else 0\n",
    ")\n",
    "\n",
    "# Step 6: Add section length metrics for potentially important sections\n",
    "for section in ['CHIEF_COMPLAINT', 'HISTORY_OF_PRESENT_ILLNESS', 'ASSESSMENT', 'DISCHARGE_DIAGNOSIS']:\n",
    "    section_col = f'section_{section}'\n",
    "    # Calculate character length \n",
    "    base_results[f'{section_col}_length'] = base_results[section_col].apply(\n",
    "        lambda x: len(x) if isinstance(x, str) else 0\n",
    "    )\n",
    "    # Calculate word count\n",
    "    base_results[f'{section_col}_words'] = base_results[section_col].apply(\n",
    "        lambda x: len(x.split()) if isinstance(x, str) else 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6f432-6a0a-4e1e-a891-a41d4a3829fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove uneccesarry columns\n",
    "results = base_results.drop(columns=['key', 'diagnoses', 'icd9_codes', 'summary_snippet', 'diagnosis_count'])\n",
    "\n",
    "# Print sample sections to verify extraction\n",
    "sample_idx = min(5, len(results)-1)  # Get a valid index\n",
    "print(f\"\\nSample sections from record {sample_idx}:\")\n",
    "for section in key_sections:\n",
    "    section_text = results.loc[sample_idx, f'section_{section}']\n",
    "    if isinstance(section_text, str) and len(section_text.strip()) > 0:\n",
    "        # Print just the first 100 characters of each section for readability\n",
    "        print(f\"{section}: {section_text[:100]}...\")\n",
    "\n",
    "# #Print the results of the summary and which codes were assocated with it Example\n",
    "# print(results.loc[5, 'summary_snippet'])\n",
    "# print(\"-\" * 20)  # Separator for clarity\n",
    "\n",
    "# icd9_codes = results.loc[1, 'icd9_codes'].split(', ')\n",
    "# diagnoses = results.loc[1, 'diagnoses'].split('; ')\n",
    "\n",
    "# for i in range(min(len(icd9_codes), len(diagnoses))):\n",
    "#   print(f\"{icd9_codes[i]} --> {diagnoses[i]}\")\n",
    "\n",
    "# # handling the case where icd9_codes and diagnoses are different lengths.\n",
    "# if len(icd9_codes) > len(diagnoses):\n",
    "#     for i in range(len(diagnoses), len(icd9_codes)):\n",
    "#         print(f\"{icd9_codes[i]} --> No corresponding diagnosis\")\n",
    "# elif len(diagnoses) > len(icd9_codes):\n",
    "#     for i in range(len(icd9_codes), len(diagnoses)):\n",
    "#         print(f\"No corresponding ICD9 --> {diagnoses[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d23fd52-363d-4ca3-8a61-990494873ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV\n",
    "#summary_results_path = os.path.join(\"C:\\\\Users\", username, \"Documents\", \"GitHub\", \"DEEP_ML_Project\", \"data\", \"preprocessed\", \"summary_results.csv\")\n",
    "#summary_results_trimmed_path = os.path.join(\"C:\\\\Users\", username, \"Documents\", \"GitHub\", \"DEEP_ML_Project\", \"data\", \"preprocessed\", \"summary_results_trimmed.csv\")\n",
    "\n",
    "# Export function that handles large files properly\n",
    "def export_to_csv(df, file_path, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Export DataFrame to CSV with proper handling of large files\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to export\n",
    "        file_path: Path for the output file\n",
    "        chunk_size: If provided, split into chunks of this size\n",
    "    \"\"\"\n",
    "    if chunk_size is None:\n",
    "        # Export as a single file with minimal options to ensure correct handling\n",
    "        df.to_csv(file_path, index=False, quoting=1)  # quoting=1 ensures proper text field handling\n",
    "        return [file_path]\n",
    "    else:\n",
    "        # Split into multiple files if chunk_size is specified\n",
    "        file_base = file_path.stem\n",
    "        file_ext = file_path.suffix\n",
    "        file_dir = file_path.parent\n",
    "        \n",
    "        chunk_files = []\n",
    "        num_chunks = (len(df) + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min((i + 1) * chunk_size, len(df))\n",
    "            chunk_file = file_dir / f\"{file_base}_part{i+1}{file_ext}\"\n",
    "            df.iloc[start_idx:end_idx].to_csv(chunk_file, index=False, quoting=1)\n",
    "            chunk_files.append(chunk_file)\n",
    "            \n",
    "        return chunk_files\n",
    "\n",
    "chunk_size = None  # Set to a value like 10000 if you want to split the file\n",
    "\n",
    "preprocessed_dir = Path(\"../data/preprocessed\")\n",
    "preprocessed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "summary_results_path = preprocessed_dir / \"summary_results.csv\"\n",
    "summary_results_trimmed_path = preprocessed_dir / \"summary_results_trimmed.csv\"\n",
    "\n",
    "#results.iloc[:-20].to_csv(summary_results_path, index=False)\n",
    "export_files = export_to_csv(results.iloc[:-20], summary_results_path, chunk_size)\n",
    "results.iloc[-20:].to_csv(summary_results_trimmed_path, index=False, quoting=1)\n",
    "\n",
    "print(f\"Full training results exported to: {summary_results_path}\")\n",
    "print(f\"Trimmed training results exported to: {summary_results_trimmed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f624d4-d992-4559-968c-529b34f25065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac3ff1-1a95-4204-b1b4-7781bae9b791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
