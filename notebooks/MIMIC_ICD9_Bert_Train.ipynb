{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "#!pip install -r https://raw.githubusercontent.com/HeedfulMoss/DEEP_ML_Project/main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d73e7-2ff7-42ca-98cd-68ea52c8a20b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device Name: NVIDIA GeForce GTX 1080 Ti\n",
      "torch version: 2.6.0+cu126\n",
      "torch location: C:\\Users\\Alex\\Documents\\GitHub\\DEEP_ML_Project\\venv\\Lib\\site-packages\\torch\\__init__.py\n",
      "Using device: cuda\n",
      "GPU device name: NVIDIA GeForce GTX 1080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_17024\\2012459533.py:93: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3871, 'grad_norm': 0.7656834721565247, 'learning_rate': 4.852148148148149e-05, 'epoch': 0.08888888888888889}\n",
      "{'loss': 0.337, 'grad_norm': 0.8874655365943909, 'learning_rate': 4.7040000000000004e-05, 'epoch': 0.17777777777777778}\n",
      "{'loss': 0.3256, 'grad_norm': 0.6774147152900696, 'learning_rate': 4.555851851851852e-05, 'epoch': 0.26666666666666666}\n",
      "{'loss': 0.3189, 'grad_norm': 0.6823941469192505, 'learning_rate': 4.407703703703704e-05, 'epoch': 0.35555555555555557}\n",
      "{'loss': 0.3223, 'grad_norm': 0.8993301391601562, 'learning_rate': 4.2595555555555554e-05, 'epoch': 0.4444444444444444}\n",
      "{'loss': 0.3154, 'grad_norm': 0.8177610635757446, 'learning_rate': 4.111407407407408e-05, 'epoch': 0.5333333333333333}\n",
      "{'loss': 0.3103, 'grad_norm': 0.7148011326789856, 'learning_rate': 3.9632592592592594e-05, 'epoch': 0.6222222222222222}\n",
      "{'loss': 0.3039, 'grad_norm': 1.087775707244873, 'learning_rate': 3.815111111111112e-05, 'epoch': 0.7111111111111111}\n",
      "{'loss': 0.2974, 'grad_norm': 0.7237125635147095, 'learning_rate': 3.6669629629629634e-05, 'epoch': 0.8}\n",
      "{'loss': 0.2918, 'grad_norm': 0.8466672897338867, 'learning_rate': 3.518814814814815e-05, 'epoch': 0.8888888888888888}\n",
      "{'loss': 0.2939, 'grad_norm': 0.8119593858718872, 'learning_rate': 3.370666666666667e-05, 'epoch': 0.9777777777777777}\n",
      "{'loss': 0.2801, 'grad_norm': 0.7180024981498718, 'learning_rate': 3.2225185185185184e-05, 'epoch': 1.0666666666666667}\n",
      "{'loss': 0.2782, 'grad_norm': 0.9228461980819702, 'learning_rate': 3.074370370370371e-05, 'epoch': 1.1555555555555554}\n",
      "{'loss': 0.2795, 'grad_norm': 0.8353630900382996, 'learning_rate': 2.9262222222222224e-05, 'epoch': 1.2444444444444445}\n",
      "{'loss': 0.2795, 'grad_norm': 0.6513243913650513, 'learning_rate': 2.778074074074074e-05, 'epoch': 1.3333333333333333}\n",
      "{'loss': 0.2807, 'grad_norm': 2.7162680625915527, 'learning_rate': 2.6299259259259264e-05, 'epoch': 1.4222222222222223}\n",
      "{'loss': 0.2766, 'grad_norm': 1.0486693382263184, 'learning_rate': 2.481777777777778e-05, 'epoch': 1.511111111111111}\n",
      "{'loss': 0.2756, 'grad_norm': 1.0093097686767578, 'learning_rate': 2.3336296296296297e-05, 'epoch': 1.6}\n",
      "{'loss': 0.2731, 'grad_norm': 0.6618999242782593, 'learning_rate': 2.1854814814814817e-05, 'epoch': 1.6888888888888889}\n",
      "{'loss': 0.2776, 'grad_norm': 0.9701287746429443, 'learning_rate': 2.0373333333333334e-05, 'epoch': 1.7777777777777777}\n",
      "{'loss': 0.2779, 'grad_norm': 1.0513864755630493, 'learning_rate': 1.8891851851851854e-05, 'epoch': 1.8666666666666667}\n",
      "{'loss': 0.2745, 'grad_norm': 1.0795834064483643, 'learning_rate': 1.741037037037037e-05, 'epoch': 1.9555555555555557}\n",
      "{'loss': 0.2687, 'grad_norm': 1.1627992391586304, 'learning_rate': 1.592888888888889e-05, 'epoch': 2.0444444444444443}\n",
      "{'loss': 0.2599, 'grad_norm': 1.589188575744629, 'learning_rate': 1.4447407407407407e-05, 'epoch': 2.1333333333333333}\n",
      "{'loss': 0.2628, 'grad_norm': 0.6800062656402588, 'learning_rate': 1.2965925925925925e-05, 'epoch': 2.2222222222222223}\n",
      "{'loss': 0.263, 'grad_norm': 0.8277254104614258, 'learning_rate': 1.1484444444444445e-05, 'epoch': 2.311111111111111}\n",
      "{'loss': 0.2611, 'grad_norm': 1.89925217628479, 'learning_rate': 1.0002962962962964e-05, 'epoch': 2.4}\n",
      "{'loss': 0.2574, 'grad_norm': 1.2973225116729736, 'learning_rate': 8.521481481481482e-06, 'epoch': 2.488888888888889}\n",
      "{'loss': 0.2581, 'grad_norm': 1.052908182144165, 'learning_rate': 7.04e-06, 'epoch': 2.5777777777777775}\n",
      "{'loss': 0.2571, 'grad_norm': 1.1741911172866821, 'learning_rate': 5.558518518518519e-06, 'epoch': 2.6666666666666665}\n",
      "{'loss': 0.2545, 'grad_norm': 0.9688524007797241, 'learning_rate': 4.077037037037038e-06, 'epoch': 2.7555555555555555}\n",
      "{'loss': 0.2582, 'grad_norm': 0.9605463743209839, 'learning_rate': 2.5955555555555558e-06, 'epoch': 2.8444444444444446}\n",
      "{'loss': 0.2587, 'grad_norm': 0.7714376449584961, 'learning_rate': 1.1140740740740741e-06, 'epoch': 2.9333333333333336}\n",
      "{'train_runtime': 8878.5351, 'train_samples_per_second': 15.205, 'train_steps_per_second': 1.901, 'train_loss': 0.28485156476056134, 'epoch': 3.0}\n",
      "\n",
      "Example Inference:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 127\u001b[39m\n\u001b[32m    125\u001b[39m example_text = df.iloc[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33msummary_snippet\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    126\u001b[39m example_true_labels = df.iloc[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33micd9_codes\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m predicted = \u001b[43mpredict_icd9\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredicted ICD9 codes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrue ICD9 codes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_true_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mpredict_icd9\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    117\u001b[39m     outputs = model(**inputs)\n\u001b[32m    118\u001b[39m     probs = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m predicted_labels = \u001b[43mmlb\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_labels\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\DEEP_ML_Project\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:934\u001b[39m, in \u001b[36mMultiLabelBinarizer.inverse_transform\u001b[39m\u001b[34m(self, yt)\u001b[39m\n\u001b[32m    919\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform the given indicator matrix into label sets.\u001b[39;00m\n\u001b[32m    920\u001b[39m \n\u001b[32m    921\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    930\u001b[39m \u001b[33;03m    `classes_[j]` for each `yt[i, j] == 1`.\u001b[39;00m\n\u001b[32m    931\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    932\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m934\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43myt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m[\u001b[32m1\u001b[39m] != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.classes_):\n\u001b[32m    935\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    936\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected indicator for \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m classes, but got \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    937\u001b[39m             \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.classes_), yt.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    938\u001b[39m         )\n\u001b[32m    939\u001b[39m     )\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sp.issparse(yt):\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# NLP Multi-label Classification Training Pipeline for ICD-9 Codes\n",
    "# Using BERT (or Bio_ClinicalBERT) + HuggingFace Transformers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import transformers\n",
    "from transformers.training_args import TrainingArguments\n",
    "import joblib\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\")\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"torch location:\", torch.__file__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# 1. Load the dataset (full and reduced versions)\n",
    "preprocessed_dir = Path(\"../data/preprocessed\")\n",
    "preprocessed_dir.mkdir(parents=True, exist_ok=True)\n",
    "train_df_path = preprocessed_dir / \"summary_results.csv\"\n",
    "df = pd.read_csv(train_df_path)\n",
    "#df = pd.read_csv(\"summary_results.csv\").sample(3000, random_state=42)\n",
    "\n",
    "# 2. Preprocessing: clean `summary_snippet`\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df[\"summary_snippet\"] = df[\"summary_snippet\"].astype(str).apply(clean_text)\n",
    "\n",
    "# 3. Convert `icd9_codes` to list and one-hot encode\n",
    "labels = df[\"icd9_codes\"].apply(lambda x: x.split(\", \"))\n",
    "mlb = MultiLabelBinarizer()\n",
    "label_matrix = mlb.fit_transform(labels)\n",
    "\n",
    "# 4. Train/val split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"summary_snippet\"].tolist(), label_matrix, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# 5. Tokenization (using Bio_ClinicalBERT or BERT)\n",
    "MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"  # or \"bert-base-uncased\" \n",
    "#MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# 6. Custom Dataset\n",
    "class ICD9Dataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ICD9Dataset(train_encodings, train_labels)\n",
    "val_dataset = ICD9Dataset(val_encodings, val_labels)\n",
    "\n",
    "# 7. Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=label_matrix.shape[1]\n",
    ")\n",
    "\n",
    "results_dir = Path(\"../models/bert_icd9/results\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "logging_dir = results_dir / \"logs\"\n",
    "\n",
    "# 8. Training args (minimal args to fix compatibility issue)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(results_dir),\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=str(logging_dir)\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 9. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 10. Train\n",
    "trainer.train()\n",
    "\n",
    "model_dir = Path(\"models/bert_icd9/icd9_bert_model\")\n",
    "binarizer_path = Path(\"models/bert_icd9/icd9_label_binarizer.pkl\")\n",
    "#results_dir = Path(\"results/bert_icd9\")\n",
    "\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "binarizer_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 11. Save model and label binarizer\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "joblib.dump(mlb, binarizer_path)\n",
    "\n",
    "# 13. Streamlit App Example (optional UI)\n",
    "# Save this in a separate file named app.py and run with: streamlit run app.py\n",
    "\"\"\"\n",
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import joblib\n",
    "\n",
    "st.title(\"ICD-9 Code Predictor from Discharge Summary\")\n",
    "\n",
    "user_input = st.text_area(\"Paste a clinical note:\", height=300)\n",
    "\n",
    "if st.button(\"Predict\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"./icd9_bert_model\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"./icd9_bert_model\")\n",
    "    mlb = joblib.load(\"icd9_label_binarizer.pkl\")\n",
    "\n",
    "    inputs = tokenizer(user_input, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()\n",
    "        predicted = mlb.inverse_transform([probs > 0.5])[0]\n",
    "\n",
    "    st.subheader(\"Predicted ICD-9 Codes:\")\n",
    "    st.write(predicted)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef0805d-5b96-4674-b542-7e125432f529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Inference:\n",
      "Admission Date: [**2113-9-10**]    Discharge Date: [**2113-9-14**]  Service: [**Hospital Ward Name **]/MICU/SICU. HISTORY OF THE PRESENT ILLNESS: Mr. [**Known lastname 19388**] is an 81-year-old man recently discharged from [**Hospital1 188**] on [**2113-9-6**] after a 27 days hospitalization. Recent hospitalization was for a mitral valve replacement and a tricuspid valve repair, which resulted in a prolonged and PEG tube placement. Additionally, during that hospitalization, he developed a right sided loculated effusion for which he underwent a videoscopic thorascopy for removal. Of note, he was sent out on a course of Vancomycin for MRSA positive sputum culture on [**2113-8-22**]. Since the discharge from [**Hospital1 69**] four days ago, Mr. [**Known lastname 19388**] has had ongoing respiratory complained of shortness of breath with purulent sputum. He was cultured. On [**2113-9-9**], he developed leukocytosis to 14. Ceftazidime and Levofloxacin were added empirically for gram-negative coverage after a culture grew gram-negative rods. On [**9-10**], the day of admission, he had increasing vent requirements. ABG at that time revealed 7.25, 71, and 75 on 70% FIO2. He was transferred to [**Hospital1 1444**] for further management. In the emergency department, he was noted to have a cuff leak, which was subsequently repaired. Additionally, he was bronchoscoped by CT surgery revealed only mural secretions. PAST MEDICAL HISTORY: 1. Coronary artery disease status post CABG in [**2097**]. 2. Atrial fibrillation on Coumadin. 3. Prostate cancer status post prostatectomy in [**2108**]. 4. Colon cancer status post colectomy. 5. Pancytopenia most likely from myelodysplasia. 6. The patient had nondiagnostic bone narrow in [**7-/2113**] on a preoperative for surgery. At that time, CBC with white count was 3.8, hematocrit 38.4, platelet count 148.000. 7. History of heparin-induced thrombocytopenia. Antibodies positive in [**2113-8-13**]. PAST PSYCHIATRIC HISTORY: 1. Status post coronary artery bypass graft times two in [**2097**]. 2. Status post colonoscopy in [**2109**]. 3. Status post colectomy in [**2107**]. 4. Status post radial prostatectomy in [**2108**]. 5. Status post penile implant. 6. Status post right urethral stent with ileostomy. 7. Status post bone narrow biopsy that diagnosis pancytopenia in [**2113-7-13**]. ALLERGIES: No known drug allergies. MEDICATIONS ON ADMISSION: 1. Prozac 20 mg per G tube q.d. 2. Digoxin 0.25 per G tube q.d. 3. Protonix 40 mg per G tube q.d. 4. Promote with fiber 75 cc an hour. 5. Free water boluses 250 cc q.i.d. 6. Lasix 20 mg per G tube q.d. 7. Levofloxacin. 8. Vancomycin. 9. Ceftazidime. 10. Simethicone 80 q.i.d. 11. Coumadin. FAMILY HISTORY: The patient's family history revealed the following: Father died of a myocardial infarction at the age of 59. Mother died of breast cancer at the age of 48. SOCIAL HISTORY: Prior to the CT surgery, he lived alone in [**Location (un) **], NH. The patient has positive tobacco use four packs per day for 37 years. He quit 35 years ago. Alcohol use about ten beers per month. He denies any other drug use. PHYSICAL EXAMINATION: Examination on admission revealed the following: Pulse 85, blood pressure 135/86, oxygen saturation 97%, temperature 99.1. On the vent he was in a pressure-support mode with pressure support of 8, PEEP of 5, and FIO2 of 50%. GENERAL: The tracheostomy was in place. He was alert. He nodes to answers questions. HEENT: Unremarkable. LUNGS: Coarse breath sounds throughout, left greater than right. CARDIOVASCULAR: Irregularly irregular, no murmurs, rubs, or gallops. ABDOMEN: Benign. G-tube was in place. Colostomy bag was in place. Urostomy bag was in place. BACK: No CVA tenderness. EXTREMITIES: Lower extremity had 1+ edema. SKIN: 2 cm elliptical purpuric lesion in the left cheek; venous stasis changes lower extremities. LABORATORY DATA: Labs on admission were significant for a white count of 13.1, hematocrit 29.5, platelet count 244,000, INR 3.3, BUN 45, creatinine 0.8. Gas shortly after admission revealed the pH of 7.45, pCO2 44, pO2 of 74. Chest x-ray revealed bilateral lower lobe opacities, greater on the right than the left, which had been increased compared to the prior study. The patient was status post thoracotomy, bilateral pleural effusions, greater on the right than on the left. There was cardiomegaly with mild edema. CT of the chest was done in the ED, which revealed large right lung consolidation, bilateral fluid collections, left greater than right. Fluid collection has enhancing walls and empyema could not be excluded. Compared to the prior study, the consolidation on the right is more extensive, but the fluid collection had decreased in size, although remains loculated. EKG: Atrial fibrillation at 73, normal axis, QRS, 0.125 with borderline left bundle branch block. ST depressions in V5 and V6. No Q waves, no changes from old\n",
      "Predicted ICD9 codes: ('42731', '4280')\n",
      "True ICD9 codes: 42731\n"
     ]
    }
   ],
   "source": [
    "# 12. Inference Function (for later use)\n",
    "def predict_icd9(text):\n",
    "    model.eval()\n",
    "    model.to(device)  # move model to GPU if available\n",
    "    text = clean_text(text)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256).to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "    predicted_labels = mlb.inverse_transform(probs > 0.5)[0]\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "# 14. Example Inference from Dataset\n",
    "print(\"\\nExample Inference:\")\n",
    "example_text = df.iloc[0][\"summary_snippet\"]\n",
    "example_true_labels = df.iloc[0][\"icd9_codes\"]\n",
    "predicted = predict_icd9(example_text)\n",
    "print(example_text)\n",
    "print(f\"Predicted ICD9 codes: {predicted}\")\n",
    "print(f\"True ICD9 codes: {example_true_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfbc3f-037c-4419-bddb-d6af38ae3d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
