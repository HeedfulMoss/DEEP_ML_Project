!pip install python-docx
from google.colab import drive
drive.mount('/content/drive')

# dependencies !pip install transformers torch datasets accelerate peft scikit-learn wandb

!pip install datasets

import numpy as np
import torch
from torch import nn
from sklearn.utils.class_weight import compute_class_weight
from datasets import load_dataset
from transformers import (
    LongformerTokenizer,
    LongformerForSequenceClassification,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model
from transformers.trainer_pt_utils import IterableDatasetShard

# --- Constants ---
MODEL_NAME = "allenai/longformer-base-4096"
DATA_PATH = '/content/drive/MyDrive/DEEP_ML_Project-main/data/preprocessed/summary_results.csv'  # Columns: text, ICD9_4019, ICD9_4280, ...
OUTPUT_DIR = "./longformer_icd9_finetuned"
MAX_LENGTH = 4096  # Adjust based on note length (4096 max)

# --- Load Data ---
dataset = load_dataset("csv", data_files=DATA_PATH, split="train")

# Extract all ICD-9 column names (assuming format ICD9_XXXX)
icd9_columns = [col for col in dataset.column_names if col.startswith("ICD9_")]

# Split train/validation
dataset = dataset.train_test_split(test_size=0.1, seed=42)
train_data = dataset["train"]
val_data = dataset["test"]

# --- Tokenization ---
tokenizer = LongformerTokenizer.from_pretrained(MODEL_NAME)

def tokenize(batch):
    return tokenizer(
        batch["summary_snippet_clean"],
        padding="max_length",
        truncation=True,
        max_length=MAX_LENGTH
    )

train_data = train_data.map(
    tokenize, 
    batched=True,
    batch_size=16,                  # Larger batch for tokenization
    load_from_cache_file=True,      # Avoid re-tokenization
    desc="Tokenizing train data"
)

val_data = val_data.map(
    tokenize,
    batched=True,
    batch_size=16,
    load_from_cache_file=True,
    desc="Tokenizing val data"
)

# Convert ICD9 columns to multi-hot labels
train_data = train_data.map(lambda x: {"labels": [int(x[col]) for col in icd9_columns]})
val_data = val_data.map(lambda x: {"labels": [int(x[col]) for col in icd9_columns]})

# --- Class Weighting ---
all_labels = np.array(train_data["labels"])
class_weights = compute_class_weight(
    "balanced",
    classes=np.arange(len(icd9_columns)),  # One weight per ICD-9 code
    y=all_labels.argmax(axis=1) if len(icd9_columns) > 1 else all_labels.ravel()
)
class_weights = torch.tensor(class_weights, dtype=torch.float32)

# --- Model Setup ---
model = LongformerForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=len(icd9_columns),
    problem_type="multi_label_classification"
)

# Optional: LoRA for efficient fine-tuning
lora_config = LoraConfig(
    task_type="SEQ_CLS",
    r=4,                            # Reduced rank
    lora_alpha=8,                   # Scaled-down alpha
    target_modules=["query", "value"],
    lora_dropout=0.05,
    bias="none"
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # ~1% of parameters trained

# --- Custom Loss ---
class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.pop("labels").to(torch.float32)
        outputs = model(**inputs)
        logits = outputs.logits
        
        # Vectorized weight application
        weights = class_weights.to(logits.device)[None, :].expand(logits.size(0), -1)
        loss_fct = nn.BCEWithLogitsLoss(weight=weights, reduction="mean")
        
        loss = loss_fct(logits, labels)
        return (loss, outputs) if return_outputs else loss

# --- Training Args ---
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=4,  # Increased batch size
    gradient_accumulation_steps=4,   # Reduced accumulation steps
    num_train_epochs=1,
    fp16=True,                      # Mixed precision
    tf32=True,                      # Enable TensorFloat-32 (Ampere+ GPUs)
    gradient_checkpointing=True,    # Memory/throughput trade-off
    optim="adamw_torch_fused",      # Faster AdamW implementation
    eval_strategy="steps",
    eval_steps=500,
    save_strategy="steps",
    save_steps=1000,
    logging_steps=100,
    report_to="wandb",
    dataloader_num_workers=4,
    dataloader_pin_memory=True,     # Faster data transfer to GPU
    remove_unused_columns=True,     # Reduce memory usage
)

# --- Train ---
trainer = WeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
)


trainer.train()
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"Model saved to {OUTPUT_DIR}")






# PREDICT
import torch
from transformers import LongformerTokenizer, LongformerForSequenceClassification

model_path = "./longformer_icd9_finetuned"
tokenizer = LongformerTokenizer.from_pretrained(model_path)
model = LongformerForSequenceClassification.from_pretrained(model_path)

def predict_icd9(text, threshold=0.3):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=2048)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.sigmoid(outputs.logits).squeeze()
    return [model.config.id2label[i] for i, prob in enumerate(probs) if prob > threshold]

# Example
text = "PUT YOUR PROMPT"
print(predict_icd9(text))  # Output: ["ICD9_4019", "ICD9_4280"] ? maybe

