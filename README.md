# DEEP_ML_Project

## Project Tree Summary
```
DEEP_ML_Project/
â”œâ”€â”€ .git/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€data/
â”‚   â”œâ”€â”€ raw/                         â† Original compressed files from MIMIC-III
â”‚   â”‚   â”œâ”€â”€ D_ICD_DIAGNOSES.csv.gz
â”‚   â”‚   â”œâ”€â”€ DIAGNOSES_ICD.csv.gz
â”‚   â”‚   â”œâ”€â”€ NOTEEVENTS.csv.gz       ğŸ”’ [Manual Download Required]
â”‚   â”‚
â”‚   â”œâ”€â”€ extracted/                   â† Uncompressed versions of raw files, also contains trimmed versions
â”‚   â”‚   â”œâ”€â”€ D_ICD_DIAGNOSES.csv
â”‚   â”‚   â”œâ”€â”€ DIAGNOSES_ICD.csv
â”‚   â”‚   â”œâ”€â”€ NOTEEVENTS.csv          ğŸ”’ [Manual Download Required]
â”‚   â”‚
â”‚   â””â”€â”€ preprocessed/                   â† Pre-cleaned, ML-ready data
â”‚       â”œâ”€â”€ summary_results.csv            âŒ â† Created by `Pre-Processing.ipynb`
â”‚       â”œâ”€â”€ summary_results_trimmed.csv  â† Created by `Pre-Processing.ipynb`
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ bert_icd9/                  â† directory to store files generated from MIMIC_ICD9_Bert_Train
â”‚       â”œâ”€â”€ icd9_bert_model/        âŒ â† [HuggingFace model artifacts] too large
â”‚       â”œâ”€â”€ results/                âŒ   
â”‚       â””â”€â”€ icd9_label_binarizer.pkl
â”‚   
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Pre-Processing.ipynb           â† pre-processing, creates the summary_results files
â”‚   â”œâ”€â”€ MIMIC_ICD9_Bert_Train.ipynb     â† model training, creates populates "bert_icd9" 
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ next_steps.txt
â”‚   â””â”€â”€ test_note.txt
â”‚
â”œâ”€â”€ scripts/                           â† Added execution scripts
â”‚   â”œâ”€â”€ train_icd9_predictor.py        â† For training the model
â”‚   â”œâ”€â”€ evaluate_icd9_model.py         â† For model evaluation
â”‚   â””â”€â”€ predict_icd9_codes.py          â† For generating predictions
â”‚
â””â”€â”€ venv/                       âŒ [Ignored virtual environment]
```
**Legend**  
âŒ Ignored (see `.gitignore`)  
ğŸ”’ Requires manual download (not included in repo)  

## Important setup notes:

| Path | Reason |
|------|--------|
| `venv/` | Local Python virtual environment |
| `data/raw/NOTEEVENTS.csv.gz` | Large original MIMIC data Notes file |
| `data/extracted/NOTEEVENTS.csv` | Uncompressed MIMIC data Notes CSV from raw data |
| `data/preprocessed/summary_results.csv` | Generated from `Pre-Processing.ipynb` |
| `models/bert_icd9/icd9_bert_model/` | HuggingFace model artifacts |
| `models/bert_icd9/results/` | Training logs, metrics, evaluation results |
| `.ipynb_checkpoints/` | Jupyter notebook auto-saves |

## ğŸ“– Project Overview

**DEEP_ML_Project** is an end-to-end deep learning pipeline built to train and evaluate transformer models on the MIMIC-III clinical dataset. It includes:

- ğŸ§¼ Preprocessing of clinical notes and diagnosis codes  
- ğŸ§  Fine-tuning BERT for ICD9 code classification  
- ğŸ“Š Generating model evaluation reports and predictions  
- ğŸ’¾ Organized modular output structure for easy tracking  

### Problem Statement

In healthcare, manual assignment of diagnosis/procedure codes from clinical notes is time-consuming and error-prone. This project creates an automated system that extracts de-identified sample clinical data from MIMIC-III's unstructured text and maps them to standardized ICD-9 billing codes, with the goal of:

- Reducing the manual burden on medical coders
- Improving efficiency in the healthcare reimbursement process
- Minimizing potential billing errors and complications

## ğŸ” Source Data Details

This project uses the MIMIC-III (Medical Information Mart for Intensive Care III) clinical database, which contains de-identified health data associated with ~40,000 critical care patients. Three main source files are used:

### 1. NOTEEVENTS.csv
- Contains free-text clinical notes written by healthcare providers
- Includes discharge summaries, nursing notes, physician notes, etc.
- For this project, we focus specifically on **discharge summaries**
- Example structure:
  - ROW_ID: Unique identifier for each note
  - SUBJECT_ID: Unique identifier for each patient
  - HADM_ID: Unique identifier for each hospital admission
  - CHARTDATE: Date the note was charted
  - CATEGORY: Type of note (we filter for "Discharge summary")
  - TEXT: The actual clinical note text

### 2. DIAGNOSES_ICD.csv
- Contains ICD-9 diagnosis codes assigned to each hospital admission
- Example structure:
  - ROW_ID: Unique identifier for each diagnosis record
  - SUBJECT_ID: Unique identifier for each patient
  - HADM_ID: Unique identifier for each hospital admission
  - SEQ_NUM: Sequence number of the diagnosis
  - ICD9_CODE: The ICD-9 diagnosis code

### 3. D_ICD_DIAGNOSES.csv
- Provides descriptions for ICD-9 diagnosis codes
- Example structure:
  - ROW_ID: Unique identifier
  - ICD9_CODE: The ICD-9 diagnosis code
  - SHORT_TITLE: Brief description of the diagnosis
  - LONG_TITLE: Detailed description of the diagnosis

## ğŸ§ª Preprocessing Pipeline

The preprocessing pipeline (`Pre-Processing.ipynb`) transforms raw MIMIC-III data into a format suitable for machine learning:

### 1. Data Loading and Initial Filtering
- Loads the three source files using DuckDB for efficient processing
- Filters NOTEEVENTS to include only discharge summaries
- Identifies the top 20 most frequent ICD-9 diagnosis codes

### 2. Text and Code Extraction
- Joins discharge summaries with their corresponding ICD-9 codes
- Creates a dataset linking each hospital admission to:
  - The full discharge summary text
  - All associated ICD-9 codes for that admission
  - The text descriptions of those codes

### 3. One-Hot Encoding of ICD-9 Codes
- Transforms the comma-separated list of ICD-9 codes into one-hot encoded columns
- Creates a binary feature for each of the top 20 ICD-9 codes
- Example: If admission has code "4019", the column "ICD9_4019" is set to 1, otherwise 0

### 4. Clinical Section Extraction
- Identifies and extracts key clinical sections from the discharge summaries
- Uses regular expressions to detect section headers like:
  - Chief Complaint
  - History of Present Illness
  - Discharge Diagnosis
  - Assessment
  - Hospital Course
- Creates separate columns for each extracted section
- Adds binary indicators for section presence/absence
- Computes section metrics (length, word count)

### 5. Text Preprocessing
- Standardizes de-identified elements (e.g., [**2151-7-16**] â†’ [DATE])
- Removes excessive whitespace and normalizes formatting
- Preserves clinical content while reducing noise

### 6. Creation of Weighted Clinical Text
- Combines the most diagnostically relevant sections into a focused text representation
- Prioritizes sections most likely to contain coding-relevant information

## ğŸ“Š Dataset Structure

The final preprocessed dataset (`summary_results.csv`) contains:

### Patient Identifiers
- SUBJECT_ID: Unique patient identifier 
- HADM_ID: Unique hospital admission identifier

### Clinical Text Features
- summary_snippet_clean: The preprocessed complete discharge summary
- clinical_weighted_text: Combined text from key diagnostic sections
- section_X: Extracted text from specific clinical sections (e.g., section_CHIEF_COMPLAINT)

### Section Metadata
- has_X: Binary indicators for section presence (e.g., has_ASSESSMENT)
- section_X_length: Character count for each section
- section_X_words: Word count for each section

### Diagnosis Code Features
- ICD9_XXXX: One-hot encoded columns for each of the top 20 ICD-9 codes
- diagnosis_count: Total number of diagnosis codes for the admission

## ğŸ§  Model Architecture

The project implements an enhanced BERT-CNN architecture for automated ICD-9 code prediction:

### Core Components
- **BERT Encoder:** Pre-trained model with 768-dimensional contextual embeddings
  - Processes clinical text to understand medical terminology in context
  - Preserves sequence information across long clinical narratives
  - Output: Sequence of vectors (one 768d vector per token)

### Multi-scale Feature Extraction
- **Three Parallel CNN Layers** process BERT's output simultaneously:
  - **Kernel=3:** Captures short medical terms (HTN, CHF, MI)
  - **Kernel=4:** Captures medium phrases (atrial fibrillation)
  - **Kernel=5:** Captures longer diagnostic expressions (congestive heart failure)
  - Each CNN outputs 128 feature maps â†’ 384 total features after concatenation

### Document Structure Analysis
- **Section Features:** Extracts key diagnostic sections (64d per section)
- **Binary Indicators:** Tracks section presence/absence (reduced to 32d)
- **Numerical Features:** Analyzes section lengths and word counts (64d)

### Feature Integration
- **Fusion Layer:** Dense(allâ†’256) â†’ ReLU â†’ Dropout
  - Learns interactions between text content and document structure
  - Creates unified document representation focused on diagnostic information

### Multi-label Classification
- **Output Layer:** Linear layer (256â†’20) with sigmoid activation
- **Prediction:** Independent probability for each of the top 20 ICD-9 codes

### Complete Data Flow
```
Clinical Text Input
       â†“
BERT Embeddings (768d per token)
       â†“
       â”œâ”€â†’ CNN(k=3) â†’â”
       â”œâ”€â†’ CNN(k=4) â†’â”¼â†’ Global Max Pooling
       â””â”€â†’ CNN(k=5) â†’â”˜
                      â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“                 â†“                 â†“                 â†“
Section Features  Binary Indicators  Numerical Features  Main Text Features
(Section CNN)     (Linearâ†’32d)       (Linearâ†’64d)       (Concatenated CNN)
    â†“                 â†“                 â†“                 â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
           Feature Fusion (Linearâ†’256d)
                      â†“
         Multi-label Classification (Linearâ†’20d)
                      â†“
               ICD-9 Code Predictions
```

## ğŸ“ˆ Model Performance

### ROC Performance Analysis
- **Strong predictive power** for key cardiac conditions:
  - Code 4280 (Congestive heart failure): AUC = 0.86
  - Code 42731 (Atrial fibrillation): AUC = 0.81
  - Code 5849 (Acute kidney failure): AUC = 0.80
- **Moderate performance** for cardiovascular diagnoses:
  - Code 41401 (Coronary atherosclerosis): AUC = 0.70
- **Poor performance** on hypertension:
  - Code 4019 (Unspecified hypertension): AUC = 0.42 (near random chance)

### Precision-Recall Performance
- Average precision scores by condition:
  - Code 4280 (Heart failure): AP = 0.83
  - Code 42731 (Atrial fibrillation): AP = 0.83
  - Code 5849 (Kidney failure): AP = 0.78
  - Code 41401 (Coronary atherosclerosis): AP = 0.53
  - Code 4019 (Hypertension): AP = 0.45

### Key Findings
- Model performs best on conditions with specific terminology in notes
- Heart failure and atrial fibrillation have clearest clinical language markers
- Common conditions like hypertension are more challenging to differentiate
- Section-specific features improved predictions for nuanced diagnoses
- Performance correlates with distinctiveness of clinical language for each condition

## ğŸ§° Running the Model

### 1. Training (`train_icd9_predictor.py`)

```bash
python train_icd9_predictor.py \
    --data_dir ../data \
    --output_dir ../models/bert_icd9 \
    --batch_size 8 \
    --epochs 3 \
    --learning_rate 2e-5 \
    --text_column clinical_weighted_text
```

Final use case:

```bash
python train_icd9_predictor.py --data_dir ../data --output_dir ../models/bert_icd9 --batch_size 8 --epochs 3 --learning_rate 2e-5 --text_column clinical_weighted_text --use_section_texts --use_binary_indicators --use_numerical_features
```


Key parameters:
- `--data_dir`: Directory containing the data
- `--output_dir`: Directory to save the model
- `--batch_size`: Batch size for training (reduce if running into memory issues)
- `--epochs`: Number of training epochs
- `--text_column`: Which text column to use (clinical_weighted_text or summary_snippet_clean)

### 2. Evaluation (`evaluate_icd9_model.py`)

```bash
python evaluate_icd9_model.py \
    --model_dir ../models/bert_icd9 \
    --test_data_path ../data/preprocessed/summary_results.csv \
    --output_dir ../models/bert_icd9/evaluation_results \
    --threshold 0.5
```

```bash
python evaluate_icd9_model.py --model_dir ../models/bert_icd9 --test_data_path ../data/preprocessed/summary_results_trimmed.csv --output_dir ../models/bert_icd9/evaluation_results --text_column summary_snippet_clean
```

Key parameters:
- `--model_dir`: Directory containing the trained model
- `--test_data_path`: Path to the test data
- `--threshold`: Classification threshold (0.5 default)

### 3. Prediction (`predict_icd9_codes.py`)

```bash
# For a single file
python predict_icd9_codes.py \
    --model_dir ../models/bert_icd9 \
    --file ../docs/test_note.txt \
    --output_dir ../predictions

# Interactive mode
python predict_icd9_codes.py \
    --model_dir ../models/bert_icd9 \
    --interactive
```

## ğŸ“¦ Requirements

Install dependencies via:

```bash
pip install -r requirements.txt
```

## Design Rationale

### 1. BERT + CNN Combination
- **Clinical Language Understanding**: BERT captures deep contextual relationships in medical terminology
- **Pattern Recognition**: CNNs excel at identifying key clinical phrases regardless of their position
- **Efficiency**: CNN layers reduce the dimensionality of BERT's output while preserving important features

### 2. Multi-kernel CNN Design
- **Varied Medical Phrases**: Different kernel sizes (3, 4, 5) capture medical terms of varying lengths
- **Complementary Features**: Each kernel size focuses on different aspects of the text
  - Kernel=3: Short medical terms (e.g., "CHF", "MI", "HTN")
  - Kernel=4: Medium-length phrases (e.g., "atrial fibrillation")
  - Kernel=5: Longer diagnostic expressions (e.g., "congestive heart failure")

### 3. Section-Specific Processing
- **Medical Document Structure**: Clinical notes are highly structured with specialized sections
- **Section Relevance**: Different sections have varying diagnostic importance
- **Targeted Feature Extraction**: Allows the model to learn section-specific patterns

### 4. Multi-label Classification Approach
- **Clinical Reality**: Patients typically have multiple conditions
- **Code Dependencies**: Some ICD-9 codes commonly co-occur (e.g., hypertension and heart failure)
- **Threshold Flexibility**: Sigmoid outputs allow for tuning precision/recall tradeoffs per code

## Top 20 ICD-9 Codes

The project focuses on predicting the 20 most common ICD-9 codes in the MIMIC-III dataset:

1. 4019: Unspecified essential hypertension
2. 4280: Congestive heart failure, unspecified
3. 42731: Atrial fibrillation
4. 41401: Coronary atherosclerosis of native coronary artery
5. 5849: Acute kidney failure, unspecified
6. 25000: Diabetes mellitus without mention of complication, type II or unspecified type
7. 2724: Other and unspecified hyperlipidemia
8. 51881: Acute respiratory failure
9. 5990: Urinary tract infection, site not specified
10. 53081: Esophageal reflux
11. 2720: Pure hypercholesterolemia
12. V053: Need for prophylactic vaccination and inoculation against viral hepatitis
13. V290: Observation for suspected infectious condition
14. 2859: Anemia, unspecified
15. 2449: Unspecified acquired hypothyroidism
16. 486: Pneumonia, organism unspecified
17. 2851: Acute posthemorrhagic anemia
18. 2762: Acidosis
19. 496: Chronic airway obstruction, not elsewhere classified
20. 99592: Severe sepsis
